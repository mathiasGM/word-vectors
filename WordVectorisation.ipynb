{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorisation meets words\n",
    "Examples of how word-vectors (word-embeddings) can be used to all sorts of things!\n",
    "\n",
    "// By MGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for finding the Cosine Similarity\n",
    "Cosine between two vectors.\n",
    "\n",
    "Similarity comes between 1 and -1.\n",
    "\n",
    "\n",
    "(1 if two words are very similar. \n",
    "-1 if two words are very dissimilar.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosSim(vector1, vector2):\n",
    "    \n",
    "    cos_sim = dot(vector1,vector2)/(norm(vector1)*norm(vector2))\n",
    "    \n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "x1 = [1,2,3]\n",
    "x2 = [1,2,5]\n",
    "cosineSim = cosSim(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity:\n",
      "0.9759000729485332\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity:\")\n",
    "print(cosineSim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectorisation\n",
    "Load pre-trained word embedding. \n",
    "\n",
    "I use Danish Word2Vec Continuous Skipgram, with each word being represented by a vector of size 100. \n",
    "\n",
    "Pre-trained word embeddings can be found here: http://vectors.nlpl.eu/repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1655836 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained word embeddings\n",
    "embeddings_index = {}\n",
    "with open('word_Embedding_DK.txt','rb') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            values = line.split()\n",
    "            word = values[0].decode('utf-8')\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except UnicodeDecodeError: \n",
    "            next(f)\n",
    "    f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "vocabSize = len(embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors\n",
    "Get a vector of a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordToVec(word):\n",
    "    wordVector = embeddings_index[word.lower()]\n",
    "    return wordVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.090582 -0.002726 -0.233673 -0.242148  0.039417 -0.03588  -0.193565\n",
      "  0.181421 -0.281298 -0.002239 -0.054147  0.770873 -1.02673  -0.196059\n",
      "  0.204133 -0.082985 -0.183909  0.185923 -0.243544  0.414395 -0.23342\n",
      "  0.558042 -0.388158 -0.141617 -0.714511 -0.353422  0.127615  0.474835\n",
      "  0.308668  1.072249 -0.237765  0.263269 -0.419263  0.325531 -0.677848\n",
      "  0.201058 -0.278782  0.230346  0.110748 -0.238589  0.314232  0.045913\n",
      " -0.557342 -0.336894 -0.519073  0.659179 -0.100333 -0.355933  0.147805\n",
      "  0.139832 -0.349028 -0.104093 -0.23965  -0.105693 -0.041038  0.052655\n",
      " -0.156822 -0.060792 -0.679549 -0.72804   0.269587  0.523499  0.021881\n",
      "  0.063605  0.346054  0.390464  0.022117 -0.566829  0.696968 -0.01182\n",
      " -0.900374 -0.660137 -0.150423 -0.058631 -0.381839  0.332458 -0.251802\n",
      " -0.587417 -0.673894 -0.576689  0.127594  0.695196 -0.433831 -0.739833\n",
      " -0.174181 -0.116684 -0.189647  0.229381  0.987568 -0.462757 -0.172011\n",
      "  0.041496 -0.127187  0.106523  0.444665  0.05065   0.552203  0.047396\n",
      "  0.447666  0.509395]\n"
     ]
    }
   ],
   "source": [
    "wordVector = wordToVec(\"rødgrød\")\n",
    "print(wordVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare two words (vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40724793\n"
     ]
    }
   ],
   "source": [
    "wordVector1 = wordToVec(\"måne\")\n",
    "wordVector2 = wordToVec(\"menneske\")\n",
    "similarity = cosSim(wordVector1, wordVector2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find most similar word (vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMostSim(wordInput):\n",
    "    \n",
    "    wordInputVector = wordToVec(wordInput)\n",
    "    \n",
    "    similarity = 0\n",
    "    mostSimWord = \"Nan\"\n",
    "    \n",
    "    for word in embeddings_index:\n",
    "        \n",
    "        wordVectorDic = wordToVec(word)        \n",
    "        \n",
    "        if ((cosSim(wordInputVector,wordVectorDic) > similarity) & (wordInputVector != wordVectorDic).all()):\n",
    "            similarity = cosSim(wordInputVector,wordVectorDic)\n",
    "            mostSimWord = word\n",
    "    \n",
    "    return mostSimWord, similarity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('vidunderlig', 0.79009515)\n"
     ]
    }
   ],
   "source": [
    "# Might take a couple of minutes\n",
    "sim = findMostSim(\"smuk\")\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top k most similar word\n",
    "Method for finding the top k most similar word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTopKMostSim(wordInput,k):\n",
    "    \n",
    "    wordInputVector = wordToVec(wordInput)\n",
    "    \n",
    "    similarity = 0\n",
    "    topKWords = []\n",
    "    \n",
    "    for word in embeddings_index:\n",
    "        \n",
    "        # Get vector of a given word\n",
    "        wordVectorDic = wordToVec(word)\n",
    "        # Output is a tuple with (word, similarity)\n",
    "        similarity = cosSim(wordInputVector,wordVectorDic)\n",
    "        \n",
    "        # If topKWords is not full, append words to it until it becomes full.\n",
    "        if (len(topKWords) < k):\n",
    "            topKWords.append((word,similarity))\n",
    "        \n",
    "        # Otherwise start comparing similarity and switch/change most sim. words\n",
    "        elif len(topKWords) == k:\n",
    "            \n",
    "            # Keep track of most dissimilar word. \n",
    "            minSimWord = 1\n",
    "            for x,y in topKWords:\n",
    "                if y < minSimWord:\n",
    "                    minSimWord = y\n",
    "            \n",
    "            # If word is more similar than most dissimilar word in topKwords, swap the words.\n",
    "            if similarity > minSimWord:\n",
    "                \n",
    "                indexMinSimWord = [x[1] for x in topKWords].index(minSimWord)\n",
    "                del topKWords[indexMinSimWord]\n",
    "                topKWords.append((word, similarity))\n",
    "        \n",
    "        else:\n",
    "            print(\"Something is wrong\")\n",
    "    \n",
    "    return topKWords  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('computer', 1.0)\n",
      "('pc', 0.855114)\n",
      "('computeren', 0.8046989)\n",
      "('pc.', 0.8696126)\n",
      "(\"pc'en\", 0.81041807)\n",
      "('mobilenhed', 0.7986685)\n",
      "('windows-computer', 0.8331629)\n",
      "('internet-browser', 0.7977846)\n",
      "('usb-forbindelse', 0.79685915)\n",
      "('mediaplayer', 0.7961525)\n",
      "('medieserver', 0.7973152)\n",
      "('center-extender', 0.7960618)\n",
      "('apple-enhed', 0.81449527)\n",
      "('windows-pc.', 0.7993573)\n",
      "('10-enhed', 0.79847455)\n"
     ]
    }
   ],
   "source": [
    "mostSimWords = findTopKMostSim(\"computer\",15)\n",
    "for word in mostSimWords:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
